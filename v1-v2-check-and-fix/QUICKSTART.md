# Quick Start Guide - API Comparison Framework

**For developers and AI assistants**

## ğŸš€ First Time Setup

```bash
cd v1-v2-check-and-fix

# Record v1 baseline behavior (this will take a few minutes)
./record-tests.sh
```

**What this does:**
- Deletes old recordings, reports, and temporary files
- Starts services in RECORD mode
- Captures V1 API behavior as "ground truth"
- Runs full comparison and generates report
- Creates recordings for fast playback mode

## ğŸ“Š Check Results

```bash
# View the report
cat reports/report_*.md | head -50

# Or check for failures
cat reports/report_*.md | grep "âŒ"
```

**Report shows:**
- âœ… Passing endpoints (V1 and V2 match)
- âŒ Failing endpoints (differences found)
- Detailed comparison of response differences

## ğŸ”§ Development Loop (Fixing Endpoints)

Once initialized, use this fast iteration cycle:

```bash
# 1. Start services and run tests (playback mode - uses cached recordings)
./play-tests.sh

# 2. Check what's failing
cat reports/report_*.md | grep "âŒ"

# 3. Fix code in rest-v2

# 4. Rebuild
cd ../prroxy/rest-v2
go build -o rest-v2 ./cmd/server
cd ../../v1-v2-check-and-fix

# 5. Restart and test again
./remove.sh
./play-tests.sh

# 6. Repeat until all endpoints pass
```

## ğŸ¤– What to Say to AI Assistants

### Initial Setup
```
"Please read the README and action
```

## ğŸ“ Key Files

| File | Purpose |
|------|---------|
| `record-tests.sh` | Record v1 baseline (first time setup) |
| `play-tests.sh` | Test v2 against v1 baseline (daily use) |
| `run-reporter.sh` | Run comparison tests manually |
| `remove.sh` | Stop all services |
| `config.*.json` | Endpoint test configurations |
| `env.linux` / `env.darwin` | OS-specific service ports and paths |
| `FIX-PROCESS.md` | **MANDATORY** TDD fix process |
| `TESTING-WORKFLOW.md` | Detailed workflow documentation |

## ğŸ¯ Common Workflows

### Fresh Capture (V1 Changed)
```bash
./record-tests.sh
```

### Fast Development (Reuse Recordings)
```bash
./play-tests.sh                               # Start + test in playback
# ... fix code ...
./remove.sh && ./play-tests.sh                # Restart + test again
```

### Record Fresh Data (Force)
```bash
PROXY_MODE=record ./play-tests.sh             # Override to record mode
```

### Test Specific Endpoints
```bash
# Create a custom config
cp config.comprehensive.json config.my-test.json
# Edit config.my-test.json to include only your endpoints
./run-reporter.sh config.my-test.json
```

## ğŸ“ Understanding the Process

**Workflow:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   record-tests.sh   â”‚  â† First time: Capture V1 behavior
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ RECORD mode: V1 responses saved
           â”œâ”€â†’ Creates recordings/
           â”œâ”€â†’ Runs tests automatically
           â””â”€â†’ Generates first report

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   play-tests.sh     â”‚  â† Development: Fast iteration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ PLAYBACK mode: Uses recordings
           â”œâ”€â†’ No external API calls
           â”œâ”€â†’ Runs tests automatically
           â””â”€â†’ Very fast testing

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   run-reporter.sh   â”‚  â† Manual test execution
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ Calls both APIs
           â”œâ”€â†’ Compares responses
           â””â”€â†’ Generates report

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FIX-PROCESS.md    â”‚  â† Fix each endpoint
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â†’ Write test (TDD)
           â”œâ”€â†’ Fix code
           â”œâ”€â†’ Rebuild & verify
           â””â”€â†’ Commit when passing
```

## âœ… Success Criteria

Your V2 API is ready when:
- âœ… All endpoints return 200 status (or expected status)
- âœ… Response data matches V1 exactly
- âœ… Report shows: "Passing: 100%, Failing: 0%"

## ğŸ”„ For New Projects

To adapt this framework for your APIs:

1. **Copy the framework** to your project
2. **Update OS-specific env file** (`env.linux` or `env.darwin`) with your service paths and ports
3. **Create config files** listing your endpoints
4. **Set custom start commands** in env file if needed (e.g., `REST_V1_START_COMMAND="./gradlew run"`)
5. **Run `./record-tests.sh`** to capture your V1 behavior
6. **Follow the fix process** in FIX-PROCESS.md

See the main README for detailed configuration instructions.

---

**Remember:** The framework is API-agnostic. It works with any REST API as long as you configure the endpoints and service locations!
